{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, num_samples):\n",
    "        self.gain = None\n",
    "        self.num_samples = num_samples\n",
    "        self.predicted_class = None\n",
    "        self.feature = None\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class DecisionTreeClassifiers(object):\n",
    "    def __init__(self, samp_feat_attr, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.samp_feat_attr = samp_feat_attr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "        return self.tree_\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        node = Node(\n",
    "            num_samples=y.size,\n",
    "        )\n",
    "\n",
    "        # Split recursively until maximum depth is reached.\n",
    "        if depth < self.max_depth:\n",
    "            best_feature, best_split, best_gain, l_x_train, r_x_train, l_y_train, r_y_train, max_class_cnt_idx = self._best_split(self.samp_feat_attr,X,y,depth)\n",
    "            if best_feature is not None:\n",
    "                node.feature = best_feature\n",
    "                node.threshold = best_split\n",
    "                node.gain = best_gain\n",
    "                node.left = self._grow_tree(l_x_train, l_y_train, depth + 1)\n",
    "                node.right = self._grow_tree(r_x_train, r_y_train, depth + 1)\n",
    "            else:\n",
    "                if y.size == 1:\n",
    "                    node.feature = None\n",
    "                    node.predicted_class = y[y.columns[-1]].iloc[0]\n",
    "                else:\n",
    "                    node.feature = None\n",
    "                    node.predicted_class = max_class_cnt_idx\n",
    "        return node\n",
    "    \n",
    "    def _best_split(self, samp_feat_attr, x_train, y_train, depth):\n",
    "        best_idx, best_split, best_gain = None, None, 0\n",
    "        best_l_x_train, best_r_x_train, best_l_y_train, best_r_y_train = None, None, None, None\n",
    "        \n",
    "        #If input row count is 1 then no need to find gain\n",
    "        if y_train.size <= 1:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        \n",
    "        overall = y_train[y_train.columns[0]].value_counts()\n",
    "        max_class_cnt = None\n",
    "        max_class_cnt_idx = None\n",
    "        #If depth reaches max depth then calculate the max count for the class and\n",
    "        #return as the predicted class\n",
    "        if depth == self.max_depth-1:\n",
    "            for val, cnt in overall.iteritems():\n",
    "                if max_class_cnt is None:\n",
    "                    max_class_cnt = cnt\n",
    "                    max_class_cnt_idx = val \n",
    "                elif cnt >= max_class_cnt:\n",
    "                    max_class_cnt = cnt\n",
    "                    max_class_cnt_idx = val\n",
    "            return None, None, None, None, None, None, None, max_class_cnt_idx\n",
    "\n",
    "        for  i in range(len(samp_feat_attr)):\n",
    "            m = x_train[samp_feat_attr[i]].agg(np.mean)\n",
    "            df_less = x_train[samp_feat_attr[i]] < m\n",
    "            df_greater = x_train[samp_feat_attr[i]] >= m\n",
    "            less_class_count = y_train[df_less][y_train.columns[0]].value_counts()\n",
    "            greater_class_count = y_train[df_greater][y_train.columns[0]].value_counts()\n",
    "            l_x_train, r_x_train = x_train[df_less], x_train[df_greater]\n",
    "            l_y_train, r_y_train = y_train[df_less], y_train[df_greater]\n",
    "\n",
    "            #calculate left branch impurity\n",
    "            l_overall = 0\n",
    "            leftsum = 0\n",
    "            for val, cnt in less_class_count.iteritems():\n",
    "                l_overall += cnt\n",
    "            for val, cnt in less_class_count.iteritems():\n",
    "                leftsum += (cnt/l_overall)**2\n",
    "            l_impurity = 1-leftsum\n",
    "\n",
    "            #calculate right branch impurity\n",
    "            r_overall = 0\n",
    "            rightsum = 0\n",
    "            for val, cnt in greater_class_count.iteritems():\n",
    "                r_overall += cnt\n",
    "            for val, cnt in greater_class_count.iteritems():\n",
    "                rightsum += (cnt/r_overall)**2 \n",
    "            r_impurity = 1-rightsum\n",
    "\n",
    "            #overall gain\n",
    "            gain = 1 - ((l_overall/(l_overall+r_overall))*l_impurity + (r_overall/(l_overall+r_overall)*r_impurity))\n",
    "            if(gain > best_gain):\n",
    "                best_gain = gain\n",
    "                best_split = m\n",
    "                best_idx = samp_feat_attr[i]\n",
    "                best_l_x_train, best_r_x_train = l_x_train, r_x_train\n",
    "                best_l_y_train, best_r_y_train = l_y_train, r_y_train\n",
    "        return best_idx, best_split, best_gain, best_l_x_train, best_r_x_train, best_l_y_train, best_r_y_train, None\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_row = []\n",
    "        for index, row in X.iterrows():\n",
    "            pred_class = self._predict(row)\n",
    "            predicted_row.append(pred_class)\n",
    "        return predicted_row\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node:\n",
    "            if node.feature is None:\n",
    "                return node.predicted_class\n",
    "            if inputs[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "To check the Random Forest accuracy pass the output of the Random forest along with y_test,\n",
    "This will check the accuracy of predicted classes of all decision trees to that of ground truth\n",
    "\"\"\"\n",
    "def RF_Accuracy(predicted_classes, y_test):\n",
    "    class_col_name = None\n",
    "    class_col_content = []\n",
    "    for (columnName, columnData) in y_test.iteritems():\n",
    "        class_col_name = columnName\n",
    "        class_col_content = columnData.values\n",
    "        print(\"Ground truth of Xtest\")\n",
    "    print(class_col_content)\n",
    "    accuracy_cnt = 0\n",
    "    \n",
    "    max_vote_classes = []\n",
    "    for num in range(0,y_test.shape[0]):\n",
    "        max_vote = Counter(predicted_classes[:,num]).most_common(1)\n",
    "        if max_vote[0][0] == class_col_content[num]:\n",
    "            accuracy_cnt += 1\n",
    "        max_vote_classes.append(max_vote[0][0])\n",
    "    print('Model prediction Accuracy')\n",
    "    print((accuracy_cnt/y_test.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train,Y_train,X_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    Y_train = pd.DataFrame(Y_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    init = datetime.datetime.now()\n",
    "    n_trees = 10 #number of decision trees in random forest\n",
    "    tree_predcted_class_list = []\n",
    "    print('Program running........')   \n",
    "    for num in range(0,n_trees):\n",
    "        sample_feature_size = round(math.sqrt(X_train.shape[1]))\n",
    "        samp_feat_attr=random.sample(range(0,X_train.shape[1]),sample_feature_size)\n",
    "        print('feature attributes sampled for decision tree %s '%(num+1))\n",
    "        print(samp_feat_attr)\n",
    "        clf = DecisionTreeClassifiers(samp_feat_attr,max_depth = 9)\n",
    "        tree_ = clf.fit(X_train, Y_train)\n",
    "        predicted_row = clf.predict(X_test)\n",
    "        tree_predcted_class_list.append(predicted_row)\n",
    "    final = datetime.datetime.now()\n",
    "    print(\"Program starting time\")\n",
    "    print(init)\n",
    "    print(\"Program ending time\")\n",
    "    print(final)\n",
    "    print(\"Predicted class for 10 different trees\")\n",
    "    predicted_classes = np.array(tree_predcted_class_list)\n",
    "    print(predicted_classes)\n",
    "\n",
    "    print('Program terminated........')\n",
    "    return predicted_classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program running........\n",
      "feature attributes sampled for decision tree 1 \n",
      "[39, 29, 26, 44, 36, 19, 18]\n",
      "feature attributes sampled for decision tree 2 \n",
      "[33, 27, 1, 21, 14, 40, 11]\n",
      "feature attributes sampled for decision tree 3 \n",
      "[14, 44, 7, 24, 30, 39, 35]\n",
      "feature attributes sampled for decision tree 4 \n",
      "[29, 44, 18, 7, 42, 32, 41]\n",
      "feature attributes sampled for decision tree 5 \n",
      "[40, 23, 18, 2, 17, 41, 7]\n",
      "feature attributes sampled for decision tree 6 \n",
      "[7, 41, 13, 36, 3, 32, 35]\n",
      "feature attributes sampled for decision tree 7 \n",
      "[12, 2, 46, 17, 41, 39, 24]\n",
      "feature attributes sampled for decision tree 8 \n",
      "[0, 46, 47, 19, 35, 17, 34]\n",
      "feature attributes sampled for decision tree 9 \n",
      "[10, 14, 4, 15, 0, 36, 43]\n",
      "feature attributes sampled for decision tree 10 \n",
      "[20, 34, 28, 31, 16, 9, 47]\n",
      "Program starting time\n",
      "2020-03-09 16:22:43.959935\n",
      "Program ending time\n",
      "2020-03-09 16:24:03.276261\n",
      "Predicted class for 10 different trees\n",
      "[[ 3 10  6 ...  9  4  2]\n",
      " [ 3 10  8 ...  9  4  2]\n",
      " [ 3  2  4 ...  9  4  2]\n",
      " ...\n",
      " [ 1 10  5 ...  9  4  2]\n",
      " [ 3 10  8 ...  9  4  2]\n",
      " [ 3 10  8 ...  9  4  2]]\n",
      "Program terminated........\n",
      "Ground truth of Xtest\n",
      "[ 3 10  8 ...  9  4  2]\n",
      "Model prediction Accuracy\n",
      "94.48174826028568\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "features = pd.read_csv('Downloads/data.csv')\n",
    "train_samp_data = features.sample(n = round((features.shape[0]*80)/100), replace = True)\n",
    "y_train = train_samp_data.iloc[:,-1].to_frame()\n",
    "x_train = train_samp_data.iloc[:, train_samp_data.columns != y_train.columns.values[0]]\n",
    "\n",
    "test_sample = features.sample(n = round((features.shape[0]*20)/100))\n",
    "y_test = test_sample.iloc[:,-1].to_frame()\n",
    "x_test = test_sample.iloc[:, test_sample.columns != y_test.columns.values[0]]    \n",
    "\n",
    "predicted_classes = RandomForest(x_train.to_numpy(),y_train.to_numpy(),x_test.to_numpy())\n",
    "RF_Accuracy(predicted_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
